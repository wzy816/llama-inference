{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbbde102",
   "metadata": {},
   "source": [
    "# llama v1 model inference\n",
    "\n",
    "## preparation\n",
    "\n",
    "Acquire weights file and save them to /mnt/LLaMa.\n",
    "\n",
    "    ├── 7B\n",
    "    │   ├── checklist.chk\n",
    "    │   ├── consolidated.00.pth\n",
    "    │   └── params.json\n",
    "    ├── tokenizer_checklist.chk\n",
    "    └── tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git submodule update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/llama-inference/llama\")\n",
    "sys.path.append(\"/mnt/llama-inference/transformers\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8dfd0",
   "metadata": {},
   "source": [
    "## approach 1. use transformers LlamaForCausalLM\n",
    "\n",
    "Convert original weights file to hf format.\n",
    "\n",
    "output /mnt/LLaMA/\n",
    "\n",
    "    ├── hf_7B\n",
    "    │   ├── config.json\n",
    "    │   ├── generation_config.json\n",
    "    │   ├── pytorch_model-00001-of-00002.bin\n",
    "    │   ├── pytorch_model-00002-of-00002.bin\n",
    "    │   ├── pytorch_model.bin.index.json\n",
    "    │   ├── special_tokens_map.json\n",
    "    │   ├── tokenizer_config.json\n",
    "    │   ├── tokenizer.json\n",
    "    │   └── tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810163ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py  --input_dir /mnt/LLaMA/ --model_size 7B --output_dir /mnt/LLaMA/hf_7B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"/mnt/LLaMA/hf_7B\", device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/mnt/LLaMA/hf_7B\")\n",
    "\n",
    "print(f\"Mem needed: {model.get_memory_footprint() / 1024 / 1024 / 1024:.2f} GB\")\n",
    "print(model.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd5830",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I believe the meaning of life in its simplest form is\",\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids, \n",
    "                         max_length=64,\n",
    "                         top_p=0,\n",
    "                         temperature=1,\n",
    "                         do_sample=True)\n",
    "\n",
    "decoded = tokenizer.batch_decode(outputs, \n",
    "                                 skip_special_tokens=True, \n",
    "                                 clean_up_tokenization_spaces=False)\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09d4be",
   "metadata": {},
   "source": [
    "## approach 2. load weights directly to llama\n",
    "\n",
    "Refer to [example.py](https://github.com/facebookresearch/llama/blob/llama_v1/example.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2459c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from llama import ModelArgs, Tokenizer, Transformer\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "\n",
    "import os\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint=torch.load(\"/mnt/LLaMA/7B/consolidated.00.pth\", map_location=\"cuda\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = Tokenizer(model_path=\"/mnt/LLaMA/tokenizer.model\")\n",
    "\n",
    "# init model args\n",
    "with open(\"/mnt/LLaMA/7B/params.json\", \"r\") as f:\n",
    "    params = json.loads(f.read())\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=512, max_batch_size=32, **params\n",
    ")\n",
    "model_args.vocab_size = tokenizer.n_words\n",
    "print(model_args)\n",
    "\n",
    "# load model\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"49954\"\n",
    "if not torch.distributed.is_initialized():\n",
    "    torch.distributed.init_process_group(\"nccl\")\n",
    "    world_size = 1 # no. pt files\n",
    "    initialize_model_parallel(world_size)\n",
    "    local_rank = 0\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    \n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87776f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prompt = \"I believe the meaning of life in its simplest form is\"\n",
    "prompt_token = tokenizer.encode(prompt, bos=True, eos=False)\n",
    "print('prompt_token', prompt_token)\n",
    "\n",
    "max_seq_len = 64\n",
    "\n",
    "tokens = torch.full((1, max_seq_len), tokenizer.pad_id).cuda().long()\n",
    "tokens[0,:len(prompt_token)] = torch.tensor(prompt_token).long()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988510ef",
   "metadata": {},
   "source": [
    "## method 1\n",
    "change start_pos so that x from 2nd iteration have shape (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from generation.py\n",
    "def sample_top_p(probs, top_p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > top_p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n",
    "\n",
    "temperature = 1\n",
    "top_p = 0\n",
    "start_pos = 0\n",
    "for i in range(len(prompt_token), max_seq_len):\n",
    "    x = tokens[:, start_pos:i]\n",
    "    logits = model.forward(x, start_pos)\n",
    "    probs = torch.softmax(logits / temperature, dim=-1)\n",
    "    next_token = sample_top_p(probs, top_p)  \n",
    "    tokens[:,i] = next_token\n",
    "    start_pos = i\n",
    "print(tokenizer.decode(tokens[0,:].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e2f49",
   "metadata": {},
   "source": [
    "## method 2\n",
    "everytime start_pos=0, ignore temperature and top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d769d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prompt_token), max_seq_len):\n",
    "    x = tokens[:, 0:i]\n",
    "    logits = model.forward(x, start_pos=0)    \n",
    "    next_token = torch.argmax(logits, dim=-1)\n",
    "    tokens[:,i] = next_token\n",
    "print(tokenizer.decode(tokens[0,:].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama]",
   "language": "python",
   "name": "conda-env-llama-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
